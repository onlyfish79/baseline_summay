{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据\n",
    "import pandas as pd\n",
    "train = pd.read_csv('Train_DataSet.csv')\n",
    "train_label = pd.read_csv('Train_DataSet_Label.csv')\n",
    "test = pd.read_csv('Test_DataSet.csv')\n",
    "sub = pd.read_csv('submit_example.csv')\n",
    "sub = sub[['id']]\n",
    "data = pd.concat([train,test])\n",
    "data = data.merge(train_label,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#简单清洗\n",
    "import jieba\n",
    "import re\n",
    "stop_words = ['的','了','个','这','那','是','在','就','去','有','还','也','与','和','及','或','或者',\n",
    "              '都','就','和','到','我','上','下','左','右','你','它','他','她']\n",
    "su='[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+'\n",
    "def split_words(words):\n",
    "    tmp_words = re.sub(su, \"\", words)\n",
    "    cutted_words = ' '.join([i for i in jieba.cut(tmp_words) if i not in stop_words])\n",
    "    return cutted_words\n",
    "def clean_w(words):\n",
    "    tmp_words = re.sub(su, \"\", words)\n",
    "    return tmp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#合并文本\n",
    "data['text'] = data['title']+data['content']\n",
    "data['texta'] = data['text'].astype(str).apply(split_words)\n",
    "data = data.drop(['title','content'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "#训练词向量\n",
    "from gensim.models import word2vec\n",
    "import os\n",
    "def train_wv(data):\n",
    "    c_w=data['text'].astype(str).apply(clean_w)\n",
    "    vac=[jieba.lcut(i) for i in c_w]\n",
    "    model = word2vec.Word2Vec(vac,min_count=1,window=5,size=200)\n",
    "    model.save('w2v_out.w2v')\n",
    "    return model\n",
    "\n",
    "w2v_model = train_wv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分数据集\n",
    "train_data = data[data.label.isin([0,1,2])]\n",
    "test_data = data[data.id.isin(test['id'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用tokenizer进行转换\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_LEN = 600\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(data['texta']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_data['texta'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test_data['texta'])\n",
    "X = pad_sequences(list_tokenized_train, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(list_tokenized_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换label\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "lb = LabelEncoder()\n",
    "train_label = lb.fit_transform(train_data[\"label\"].values)\n",
    "train_label = to_categorical(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#切分验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_vail, y_train, y_test = train_test_split(X, train_label, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#获取词向量矩阵\n",
    "EMBEDDING_DIM = 200\n",
    "import numpy as np\n",
    "word_index = tokenizer.word_index \n",
    "embedding_matrix = np.zeros((len(word_index) + 1,EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    embedding_matrix[i,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建模型\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "def get_lstm():  \n",
    "    inp = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(WORD_IDX_LEN + 1, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = LSTM(128,return_sequences=True,name='lstm_layer')(x)\n",
    "\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(512,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(3,activation='softmax')(x)\n",
    "    lstm_model = Model(inputs=inp,outputs=x)\n",
    "#     lstm_model = multi_gpu_model(lstm_model,  gpus=3)\n",
    "    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6606 samples, validate on 734 samples\n",
      "Epoch 1/10\n",
      "6606/6606 [==============================] - 130s 20ms/step - loss: 0.7328 - acc: 0.6820 - val_loss: 0.5739 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57386, saving model to word_weight_ccf.hdf\n",
      "Epoch 2/10\n",
      "6606/6606 [==============================] - 128s 19ms/step - loss: 0.6061 - acc: 0.7401 - val_loss: 0.5747 - val_acc: 0.7507\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.57386\n",
      "Epoch 3/10\n",
      "6606/6606 [==============================] - 127s 19ms/step - loss: 0.5573 - acc: 0.7579 - val_loss: 0.5324 - val_acc: 0.7616\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57386 to 0.53237, saving model to word_weight_ccf.hdf\n",
      "Epoch 4/10\n",
      "6606/6606 [==============================] - 126s 19ms/step - loss: 0.4963 - acc: 0.7838 - val_loss: 0.5498 - val_acc: 0.7548\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.53237\n",
      "Epoch 5/10\n",
      "6606/6606 [==============================] - 128s 19ms/step - loss: 0.4578 - acc: 0.8043 - val_loss: 0.6212 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53237\n",
      "Epoch 6/10\n",
      "6606/6606 [==============================] - 121s 18ms/step - loss: 0.4023 - acc: 0.8292 - val_loss: 0.5471 - val_acc: 0.7657\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53237\n"
     ]
    }
   ],
   "source": [
    "#----------训练\n",
    "from keras.callbacks import *\n",
    "file_path= \"word_weight_ccf.hdf\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "WORD_IDX_LEN = len(word_index)\n",
    "\n",
    "model = get_lstm()\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=10,verbose=1,\n",
    "                               validation_data=(X_vail,y_test),\n",
    "                               callbacks=[checkpoint,early_stopping])\n",
    "model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#提交\n",
    "def z(x):\n",
    "    if x==0:\n",
    "        return '0'\n",
    "    elif x==1:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '2'\n",
    "    \n",
    "pred_word= lb.inverse_transform(np.argmax(model.predict(X_test), 1)).reshape(-1,1)\n",
    "test_data['label'] = [i[0] for i in pred_word]\n",
    "sub = sub.merge(test_data[['id','label']],on='id',how='left')\n",
    "sub['label'] = sub['label'].apply(z)\n",
    "sub.to_csv('res_base.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
